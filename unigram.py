# -*- coding: utf-8 -*-
"""Unigram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16SPjI9T5EQksG6IALYsiD1W1XMRXJ7OQ
"""

import random
import re
from math import log
from collections import Counter

# Read text from "train.txt" file
with open('train.txt', 'r') as file:
    corpus = file.read()

# Preprocessing 1: Remove special characters (Except '.', ',', '?') and numbers
corpus = re.sub(r'[^a-zA-Z.,? ]', '', corpus)

# Preprocessing 2: Convert to lowercase
corpus = corpus.lower()

# Preprocessing 3: Tokenize the text into words
words = corpus.split()

print(words)

# Calculate unigram frequencies
word_freq = Counter(words)

#Unkown Word Handling
# Define a threshold for rare words (you can adjust this threshold)
threshold = 5

# Replace words occurring less than the threshold with <UNK>
words = ['<UNK>' if word_freq[word] < threshold else word for word in words]

# Recalculate word frequencies after handling unknown words
word_freq = Counter(words)

# Compute the probability distribution of words (unigram model)
total_words = len(words)
word_probs = {word: freq / total_words for word, freq in word_freq.items()}

# Create LaplaceSmoothing instance
class LaplaceSmoothing:
    def __init__(self, vocabulary, n_gram_counts, k=1):
        self.vocabulary = vocabulary
        self.n_gram_counts = n_gram_counts
        self.k = k

    def calculate_smoothed_probability(self, n_gram):
        # Calculate the count of the n-gram and the count of the n-gram prefix
        n_gram_count = self.n_gram_counts.get(n_gram, 0)
        prefix_count = self.n_gram_counts.get(n_gram[:-1], 0)

        # Calculate the smoothed probability using Laplace smoothing
        smoothed_prob = (n_gram_count + self.k) / (prefix_count + (self.k * len(self.vocabulary)))

        return smoothed_prob

# Create AddkSmoothing instance
class AddKSmoothing:
    def __init__(self, n_gram_counts, k=1):
        self.n_gram_counts = n_gram_counts
        self.k = k

    def calculate_smoothed_probability(self, n_gram):
        # Calculate the count of the n-gram
        n_gram_count = self.n_gram_counts.get(n_gram, 0)

        # Calculate the total count of n-grams with the same prefix
        total_prefix_count = sum(count for gram, count in self.n_gram_counts.items() if gram[:-1] == n_gram[:-1])

        # Calculate the smoothed probability using Add-k smoothing
        smoothed_prob = (n_gram_count + self.k) / (total_prefix_count + (self.k * len(self.n_gram_counts)))

        return smoothed_prob

# Compute the probability distribution of words (unigram model) with Laplace smoothing
total_words = len(words)
laplace_smoothing = LaplaceSmoothing(words, word_freq, k=1)
addk_smoothing = AddKSmoothing(word_freq, k=1)

# Update word_probs to use smoothed probabilities
laplace_probs = {word: laplace_smoothing.calculate_smoothed_probability(word) for word in word_freq.keys()}
addk_probs = {word: addk_smoothing.calculate_smoothed_probability(word) for word in word_freq.keys()}
print(addk_probs)

"""
# For testing:
# Generate random sentences using the unigram model
def generate_unigram_sentence():
    sentence = []
    while True:
        word = random.choices(list(addk_probs.keys()), list(addk_probs.values()))[0]
        if word == '.':
            if len(sentence) >= 2:
                break
            else:
                continue
        sentence.append(word)
    return ' '.join(sentence)

for _ in range(5):
    print(generate_unigram_sentence())
"""


# Read text from "validation.txt" file
with open('val.txt', 'r') as file:
    validation_corpus = file.read()

# Preprocessing: Remove special characters and numbers
validation_corpus = re.sub(r'[^a-zA-Z.,? ]', '', validation_corpus)

# Convert to lowercase and tokenize the validation text into words
validation_corpus = validation_corpus.lower()
validation_words = validation_corpus.split()

# Handle unknown words in the validation data
validation_words = ['<UNK>' if word not in word_freq or word_freq[word] < threshold else word for word in validation_words]

# Calculate perplexity for the unigram model
def calculate_unsmoothed_unigram_perplexity():
    total_log_prob = 0.0
    N = len(validation_words)

    for word in validation_words:
        word_prob = word_probs.get(word, word_probs['<UNK>'])  # Use '<UNK>' for unknown words
        total_log_prob += -1 * (1 / N) * log(word_prob, 2)  # Using base-2 logarithm

    perplexity = pow(2, total_log_prob)
    return perplexity

def calculate_laplace_unigram_perplexity():
    total_log_prob = 0.0
    N = len(validation_words)

    for word in validation_words:
        word_prob = laplace_probs.get(word, laplace_probs['<UNK>'])  # Use '<UNK>' for unknown words
        total_log_prob += -1 * (1 / N) * log(word_prob, 2)  # Using base-2 logarithm

    perplexity = pow(2, total_log_prob)
    return perplexity

def calculate_addk_unigram_perplexity():
    total_log_prob = 0.0
    N = len(validation_words)

    for word in validation_words:
        word_prob = addk_probs.get(word, addk_probs['<UNK>'])  # Use '<UNK>' for unknown words
        total_log_prob += -1 * (1 / N) * log(word_prob, 2)  # Using base-2 logarithm

    perplexity = pow(2, total_log_prob)
    return perplexity

# Calculate and print perplexity for both models
unsmoothed_unigram_perplexity = calculate_unsmoothed_unigram_perplexity()
laplace_unigram_perplexity = calculate_laplace_unigram_perplexity()
addk_unigram_perplexity = calculate_addk_unigram_perplexity()

print(f"Unsmoothed Unigram Perplexity: {unsmoothed_unigram_perplexity}")
print(f"Laplace Unigram Perplexity: {laplace_unigram_perplexity}")
print(f"Add-K Unigram Perplexity: {addk_unigram_perplexity}")